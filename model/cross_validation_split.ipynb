{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028856ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20231596\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Iterable, List, Tuple, Union, Generator\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from pipeline import train_and_evaluate_model\n",
    "from data_prep.data_prep import prepare_all_data\n",
    "from model import burglary_model\n",
    "from training.training import prepare_model_data, grid_search\n",
    "\n",
    "model_tuple, occupation_mappings,ward_idx_map = prepare_all_data(\"../merged_data.parquet\", \"lsoa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b9d4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LSOA code (2021)', 'date',\n",
       "       'Index of Multiple Deprivation (IMD) Rank (where 1 is most deprived)',\n",
       "       'Income Rank (where 1 is most deprived)',\n",
       "       'Employment Rank (where 1 is most deprived)',\n",
       "       'Education, Skills and Training Rank (where 1 is most deprived)',\n",
       "       'Health Deprivation and Disability Rank (where 1 is most deprived)',\n",
       "       'Crime Rank (where 1 is most deprived)',\n",
       "       'Barriers to Housing and Services Rank (where 1 is most deprived)',\n",
       "       'Living Environment Rank (where 1 is most deprived)',\n",
       "       'Burglaries amount', 'Education locations', 'Emergency locations',\n",
       "       'Entertainment locations', 'Food locations', 'Leisure locations',\n",
       "       'Parking locations', 'Shopping locations', 'Public transport locations',\n",
       "       'Dwelling type|Flat, maisonette or apartment (%)',\n",
       "       'Ethnic Group|Asian/Asian British (%)', 'Ethnic Group|BAME (%)',\n",
       "       'Ethnic Group|Black/African/Caribbean/Black British (%)',\n",
       "       'Ethnic Group|Mixed/multiple ethnic groups (%)',\n",
       "       'Ethnic Group|Other ethnic group (%)', 'Ethnic Group|White (%)',\n",
       "       'Household Composition|% Couple household with dependent children',\n",
       "       'Household Composition|% Couple household without dependent children',\n",
       "       'Household Composition|% Lone parent household',\n",
       "       'Household Composition|% One person household',\n",
       "       'Household Composition|% Other multi person household',\n",
       "       'Households|All households', 'Mid-year Population Estimates|Aged 0-15',\n",
       "       'Mid-year Population Estimates|Aged 16-29',\n",
       "       'Mid-year Population Estimates|Aged 30-44',\n",
       "       'Mid-year Population Estimates|Aged 45-64',\n",
       "       'Mid-year Population Estimates|Aged 65+',\n",
       "       'Mid-year Population Estimates|All Ages',\n",
       "       'Mid-year Population Estimates|Working-age',\n",
       "       'Tenure|Owned outright (%)', 'Tenure|Owned with a mortgage or loan (%)',\n",
       "       'Tenure|Private rented (%)', 'Tenure|Social rented (%)',\n",
       "       'Car or van availability|1 car or van in household (%)',\n",
       "       'Car or van availability|2 cars or vans in household (%)',\n",
       "       'Car or van availability|3 cars or vans in household (%)',\n",
       "       'Car or van availability|4 or more cars or vans in household (%)',\n",
       "       'Car or van availability|Cars per household',\n",
       "       'Car or van availability|No cars or vans in household (%)',\n",
       "       'Public Transport Accessibility Levels|% 0-1 (poor access)|Level3_65',\n",
       "       'Public Transport Accessibility Levels|% 2-3 (average access)|Level3_66',\n",
       "       'Public Transport Accessibility Levels|% 4-6 (good access)|Level3_67',\n",
       "       'Public Transport Accessibility Levels|Average Score|Level3_64',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|0',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|1a',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|1b',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|2',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|3',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|4',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|5',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|6a',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|6b',\n",
       "       'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_parquet(\"../merged_data.parquet\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9a54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from calendar import month_name\n",
    "from typing import Any, Dict, List, Sequence, Tuple   # ← 2️⃣ add this\n",
    "\n",
    "def df_to_tensor_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    lsoa_col: str = \"LSOA code (2021)\",\n",
    "    occ_col:  str | None = None,\n",
    "    static_cols: Sequence[str] | None = None,\n",
    "    dynamic_cols: Sequence[str] | None = None,\n",
    "    date_col: str = \"date\",\n",
    "    target_col: str = \"Burglaries amount\",\n",
    "    drop_cols: tuple[str, ...] = (\"geometry\",)\n",
    ") -> dict[str, np.ndarray]:\n",
    "\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Empty slice passed to df_to_tensor_dict\")\n",
    "\n",
    "    # ── 1.  categorical indices ───────────────────────────────────────\n",
    "    ward_idx = df[lsoa_col].astype(\"category\").cat.codes.to_numpy(np.int64)\n",
    "    occupation_idx = (\n",
    "        np.zeros(n, dtype=np.int64) if occ_col is None or occ_col not in df\n",
    "        else df[occ_col].astype(\"category\").cat.codes.to_numpy(np.int64)\n",
    "    )\n",
    "\n",
    "    # ── 2.  choose numeric columns ────────────────────────────────────\n",
    "    numeric_cols = [c for c in df.select_dtypes(\"number\").columns if c not in drop_cols]\n",
    "\n",
    "    if static_cols is None or dynamic_cols is None:\n",
    "        varying = df[numeric_cols].nunique(dropna=False) > 1\n",
    "        inferred_dyn  = [c for c, v in varying.items() if v]\n",
    "        inferred_stat = [c for c in numeric_cols if c not in inferred_dyn]\n",
    "        dynamic_cols  = inferred_dyn  if dynamic_cols is None else list(dynamic_cols)\n",
    "        static_cols   = inferred_stat if static_cols  is None else list(static_cols)\n",
    "\n",
    "    X_static  = df[static_cols ].to_numpy(np.float32) if static_cols  else np.zeros((n, 0), np.float32)\n",
    "    X_dynamic = df[dynamic_cols].to_numpy(np.float32) if dynamic_cols else np.zeros((n, 0), np.float32)\n",
    "\n",
    "    # ── 3.  temporal features ─────────────────────────────────────────\n",
    "    months       = pd.to_datetime(df[date_col]).dt.month.to_numpy()         # 1-12\n",
    "    month_angle  = 2 * np.pi * (months - 1) / 12\n",
    "    month_sin    = np.sin(month_angle).reshape(-1, 1)                       # (n, 1)\n",
    "    month_cos    = np.cos(month_angle).reshape(-1, 1)                       # (n, 1)\n",
    "\n",
    "    # one-hot: shape (n, 12)\n",
    "    month_1h = np.eye(12, dtype=np.float32)[months - 1]\n",
    "\n",
    "    time_trend = np.arange(n, dtype=np.float32).reshape(-1, 1)\n",
    "    time_norm  = (time_trend - time_trend.mean()) / time_trend.std()\n",
    "\n",
    "    X_temporal = np.concatenate([month_sin, month_cos, time_norm], axis=1)  # (n, 3)\n",
    "\n",
    "    # ── 4.  map to model key names ─────────────────────────────────────\n",
    "    return {\n",
    "        \"occupation_idx\": torch.as_tensor(occupation_idx, dtype=torch.long),\n",
    "        \"ward_idx\":       torch.as_tensor(ward_idx,       dtype=torch.long),\n",
    "        \"X_static\":       torch.as_tensor(X_static,       dtype=torch.float32),\n",
    "        \"X_dynamic\":      torch.as_tensor(X_dynamic,      dtype=torch.float32),\n",
    "        \"X_seasonal\":     torch.as_tensor(month_1h,       dtype=torch.float32),  # ← fixed\n",
    "        \"X_time_trend\":   torch.as_tensor(time_trend,     dtype=torch.float32),  # raw trend\n",
    "        \"X_temporal\":     torch.as_tensor(X_temporal,     dtype=torch.float32),  # compact 3-col\n",
    "        \"X_spatial\":      torch.as_tensor(X_static,       dtype=torch.float32),  # alias for model\n",
    "        \"y\":              torch.as_tensor(df[target_col].to_numpy(),\n",
    "                                          dtype=torch.float32),\n",
    "        # dummy scalers so train_model never raises KeyError\n",
    "        \"means\": torch.zeros(1, dtype=torch.float32),\n",
    "        \"stds\":  torch.ones(1,  dtype=torch.float32),\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# rolling-origin CV for a DataFrame\n",
    "# --------------------------------------------------------------------\n",
    "def rolling_origin_cv(\n",
    "        \n",
    "        df: pd.DataFrame,\n",
    "        model_function,\n",
    "        inx_to_occupation_map: Dict[int, str],\n",
    "        *,\n",
    "        # ───────── parameters that control the rolling window ──────────\n",
    "        date_col: str = \"date\",\n",
    "        occ_col: str | None = None,          # ← NEW\n",
    "        static_cols: Sequence[str] | None = None,  # ← NEW (optional)\n",
    "        dynamic_cols: Sequence[str] | None = None, # ← NEW (optional)\n",
    "        initial_train_size: int | float | str | None = None,\n",
    "        horizon: int = 1,\n",
    "        step_size: int = 1,\n",
    "        expanding_window: bool = True,\n",
    "        max_splits: int | None = None,\n",
    "        # ───────── parameters forwarded to the learner ─────────────────\n",
    "        guide_type: str = \"diag\",\n",
    "        guide_rank: int = 10,\n",
    "        lr: float = 1e-3,\n",
    "        elbo_type: str = \"trace\",\n",
    "        renyi_alpha: float = 0.5,\n",
    "        num_particles: int = 1,\n",
    "        training_steps: int = 500,\n",
    "        testing_steps: int = 1000\n",
    ") -> Tuple[Dict[str, List[float]],        # aggregated metrics\n",
    "           List[Tuple[Dict[str, float],   # per-fold artefacts\n",
    "                     Any, Any, Any]]]:\n",
    "\n",
    "    if static_cols is None or dynamic_cols is None:\n",
    "        # use the *entire* data set to decide what varies\n",
    "        numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "        varying = df[numeric_cols].nunique(dropna=False) > 1\n",
    "        dynamic_cols = [c for c, v in varying.items() if v] \\\n",
    "                       if dynamic_cols is None else list(dynamic_cols)\n",
    "        static_cols  = [c for c in numeric_cols if c not in dynamic_cols] \\\n",
    "                       if static_cols  is None else list(static_cols)\n",
    "\n",
    "    # 1 ── make sure the frame is in chronological order\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "    n_obs = len(df)\n",
    "    if n_obs < 3:\n",
    "        raise ValueError(\"Need at least three rows for cross-validation.\")\n",
    "\n",
    "    # 2 ── resolve initial_train_size\n",
    "    if initial_train_size is None:\n",
    "        initial_train_size = int(np.floor(0.7 * n_obs))\n",
    "    elif isinstance(initial_train_size, str) and initial_train_size.endswith('%'):\n",
    "        pct = float(initial_train_size.rstrip('%')) / 100.0\n",
    "        initial_train_size = int(np.floor(pct * n_obs))\n",
    "    elif isinstance(initial_train_size, float) and 0 < initial_train_size < 1:\n",
    "        initial_train_size = int(np.floor(initial_train_size * n_obs))\n",
    "\n",
    "    if initial_train_size < 1 or initial_train_size >= n_obs:\n",
    "        raise ValueError(\"initial_train_size must be between 1 and len(df)-1\")\n",
    "\n",
    "    # 3 ── bookkeeping containers\n",
    "    train_start, train_end = 0, initial_train_size\n",
    "    split_idx = 0\n",
    "    per_fold_vals: Dict[str, List[float]] = {\"rmse\": [], \"mae\": [], \"crps\": []}\n",
    "    fold_results: List[Tuple[Dict[str, float], Any, Any, Any]] = []\n",
    "\n",
    "    # 4 ── main walk-forward loop\n",
    "    while train_end + horizon <= n_obs and (max_splits is None or split_idx < max_splits):\n",
    "\n",
    "        # slice DataFrame with iloc, then convert to dict of arrays\n",
    "        train_df = df.iloc[train_start:train_end].reset_index(drop=True)\n",
    "        test_df  = df.iloc[train_end:train_end + horizon].reset_index(drop=True)\n",
    "\n",
    "        train_slice = df_to_tensor_dict(\n",
    "    train_df,\n",
    "    occ_col      = occ_col,\n",
    "    static_cols  = static_cols,\n",
    "    dynamic_cols = dynamic_cols,\n",
    "    date_col     = date_col\n",
    "\n",
    ")\n",
    "        test_slice  = df_to_tensor_dict(\n",
    "    test_df,\n",
    "    occ_col      = occ_col,\n",
    "    static_cols  = static_cols,\n",
    "    dynamic_cols = dynamic_cols,\n",
    "    date_col     = date_col\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "        metrics, svi, guide, tester = train_and_evaluate_model(\n",
    "            training_data          = train_slice,\n",
    "            testing_data           = test_slice,\n",
    "            model_function         = model_function,\n",
    "            inx_to_occupation_map  = inx_to_occupation_map,\n",
    "            guide_type             = guide_type,\n",
    "            guide_rank             = guide_rank,\n",
    "            lr                     = lr,\n",
    "            elbo_type              = elbo_type,\n",
    "            renyi_alpha            = renyi_alpha,\n",
    "            num_particles          = num_particles,\n",
    "            training_steps         = training_steps,\n",
    "            testing_steps          = testing_steps\n",
    "        )\n",
    "\n",
    "        for key in per_fold_vals:\n",
    "            per_fold_vals[key].append(metrics[key])\n",
    "\n",
    "        fold_results.append((metrics, svi, guide, tester))\n",
    "\n",
    "        # shift origin\n",
    "        split_idx += 1\n",
    "        if expanding_window:\n",
    "            train_end += step_size\n",
    "        else:\n",
    "            train_start += step_size\n",
    "            train_end   += step_size\n",
    "\n",
    "    # 5 ── sanity check: at least one split produced?\n",
    "    if split_idx == 0:\n",
    "        raise ValueError(\n",
    "            f\"No CV folds were created. \"\n",
    "            f\"Check initial_train_size={initial_train_size}, horizon={horizon}, \"\n",
    "            f\"step_size={step_size}, len(df)={n_obs}.\"\n",
    "        )\n",
    "\n",
    "    # 6 ── aggregate statistics (append mean as last element)\n",
    "    for key, vals in per_fold_vals.items():\n",
    "        per_fold_vals[key].append(float(np.mean(vals)))\n",
    "\n",
    "    return per_fold_vals, fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1834953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SVI: 100%|██████████| 500/500 [00:16<00:00, 29.85it/s]\n",
      "Training SVI: 100%|██████████| 500/500 [00:17<00:00, 28.12it/s]\n",
      "Training SVI: 100%|██████████| 500/500 [00:18<00:00, 26.75it/s]\n",
      "Training SVI:   5%|▌         | 25/500 [00:02<00:39, 12.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cv_metrics, folds \u001b[38;5;241m=\u001b[39m \u001b[43mrolling_origin_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_function\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mburglary_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43minx_to_occupation_map\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moccupation_mappings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_train_size\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m60\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanding_window\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mocc_col\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# or \"your_occupation_column\"\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-fold RMSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall  RMSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cv_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 163\u001b[0m, in \u001b[0;36mrolling_origin_cv\u001b[1;34m(df, model_function, inx_to_occupation_map, date_col, occ_col, static_cols, dynamic_cols, initial_train_size, horizon, step_size, expanding_window, max_splits, guide_type, guide_rank, lr, elbo_type, renyi_alpha, num_particles, training_steps, testing_steps)\u001b[0m\n\u001b[0;32m    145\u001b[0m         train_slice \u001b[38;5;241m=\u001b[39m df_to_tensor_dict(\n\u001b[0;32m    146\u001b[0m     train_df,\n\u001b[0;32m    147\u001b[0m     occ_col      \u001b[38;5;241m=\u001b[39m occ_col,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m )\n\u001b[0;32m    153\u001b[0m         test_slice  \u001b[38;5;241m=\u001b[39m df_to_tensor_dict(\n\u001b[0;32m    154\u001b[0m     test_df,\n\u001b[0;32m    155\u001b[0m     occ_col      \u001b[38;5;241m=\u001b[39m occ_col,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m )\n\u001b[1;32m--> 163\u001b[0m         metrics, svi, guide, tester \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtesting_data\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_function\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m            \u001b[49m\u001b[43minx_to_occupation_map\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minx_to_occupation_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m            \u001b[49m\u001b[43mguide_type\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mguide_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mguide_rank\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mguide_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43melbo_type\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melbo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrenyi_alpha\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrenyi_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_particles\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_particles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtraining_steps\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtesting_steps\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtesting_steps\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m per_fold_vals:\n\u001b[0;32m    179\u001b[0m             per_fold_vals[key]\u001b[38;5;241m.\u001b[39mappend(metrics[key])\n",
      "File \u001b[1;32mc:\\Users\\20231596\\ARWCASP\\model\\pipeline.py:29\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(training_data, testing_data, model_function, inx_to_occupation_map, guide_type, guide_rank, lr, elbo_type, renyi_alpha, num_particles, training_steps, testing_steps)\u001b[0m\n\u001b[0;32m     19\u001b[0m pyro\u001b[38;5;241m.\u001b[39mclear_param_store()\n\u001b[0;32m     21\u001b[0m svi \u001b[38;5;241m=\u001b[39m create_learner(model_function,\n\u001b[0;32m     22\u001b[0m                      guide_type\u001b[38;5;241m=\u001b[39mguide_type,\n\u001b[0;32m     23\u001b[0m                      guide_rank\u001b[38;5;241m=\u001b[39mguide_rank,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m                      renyi_alpha\u001b[38;5;241m=\u001b[39mrenyi_alpha,\n\u001b[0;32m     27\u001b[0m                      num_particles\u001b[38;5;241m=\u001b[39mnum_particles)\n\u001b[1;32m---> 29\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m prediction_tester \u001b[38;5;241m=\u001b[39m PredictionTester(\n\u001b[0;32m     31\u001b[0m     testing_data, burglary_model, svi\u001b[38;5;241m.\u001b[39mguide, inx_to_occupation_map)\n\u001b[0;32m     32\u001b[0m prediction_tester\u001b[38;5;241m.\u001b[39mpredict(testing_steps)\n",
      "File \u001b[1;32mc:\\Users\\20231596\\ARWCASP\\model\\training\\training.py:178\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(data, svi, num_steps)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_steps), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining SVI\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 178\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43moccupation_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mward_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_static\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_dynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_seasonal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_time_trend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_temporal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_spatial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m svi, losses, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeans\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyro\\infer\\svi.py:145\u001b[0m, in \u001b[0;36mSVI.step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[1;32m--> 145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[0;32m    148\u001b[0m     site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyro\\infer\\trace_elbo.py:140\u001b[0m, in \u001b[0;36mTrace_ELBO.loss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide_trace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_traces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_particle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurrogate_loss_particle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_differentiable_loss_particle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide_trace\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_particle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_particles\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyro\\infer\\elbo.py:237\u001b[0m, in \u001b[0;36mELBO._get_traces\u001b[1;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_particles):\n\u001b[1;32m--> 237\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyro\\infer\\trace_elbo.py:57\u001b[0m, in \u001b[0;36mTrace_ELBO._get_trace\u001b[1;34m(self, model, guide, args, kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    against it.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     model_trace, guide_trace \u001b[38;5;241m=\u001b[39m \u001b[43mget_importance_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_plate_nesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[0;32m     61\u001b[0m         check_if_enumerated(guide_trace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyro\\infer\\enum.py:76\u001b[0m, in \u001b[0;36mget_importance_trace\u001b[1;34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[0m\n\u001b[0;32m     73\u001b[0m model_trace \u001b[38;5;241m=\u001b[39m prune_subsample_sites(model_trace)\n\u001b[0;32m     75\u001b[0m model_trace\u001b[38;5;241m.\u001b[39mcompute_log_prob()\n\u001b[1;32m---> 76\u001b[0m \u001b[43mguide_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_score_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m model_trace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyro\\poutine\\trace_struct.py:304\u001b[0m, in \u001b[0;36mTrace.compute_score_parts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# Note that ScoreParts overloads the multiplication operator\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# to correctly scale each of its three parts.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_parts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    308\u001b[0m     _, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyro\\distributions\\distribution.py:115\u001b[0m, in \u001b[0;36mDistribution.score_parts\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscore_parts\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    Computes ingredients for stochastic gradient estimators of ELBO.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    :rtype: ScoreParts\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_rsample:\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ScoreParts(\n\u001b[0;32m    118\u001b[0m             log_prob\u001b[38;5;241m=\u001b[39mlog_prob, score_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, entropy_term\u001b[38;5;241m=\u001b[39mlog_prob\n\u001b[0;32m    119\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_metrics, folds = rolling_origin_cv(\n",
    "        df                     = data,\n",
    "        model_function         = burglary_model,\n",
    "        inx_to_occupation_map  = occupation_mappings[1],\n",
    "        horizon                = 7,\n",
    "        step_size              = 10,\n",
    "        initial_train_size     = \"60%\",\n",
    "        expanding_window       = True,\n",
    "        occ_col                = None          # or \"your_occupation_column\"\n",
    ")\n",
    "\n",
    "print(\"Per-fold RMSE:\", cv_metrics[\"rmse\"][:-1])\n",
    "print(\"Overall  RMSE:\", cv_metrics[\"rmse\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00ac78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
