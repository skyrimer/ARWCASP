{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028856ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Iterable, List, Tuple, Union, Generator\n",
    "import torch\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from pipeline import train_and_evaluate_model\n",
    "from data_prep.data_prep import prepare_all_data\n",
    "from model import burglary_model\n",
    "from training.training import prepare_model_data, grid_search\n",
    "\n",
    "model_tuple, occupation_mappings,ward_idx_map = prepare_all_data(\"../merged_data.parquet\", \"lsoa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b9d4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LSOA code (2021)', 'date',\n",
       "       'Index of Multiple Deprivation (IMD) Rank (where 1 is most deprived)',\n",
       "       'Income Rank (where 1 is most deprived)',\n",
       "       'Employment Rank (where 1 is most deprived)',\n",
       "       'Education, Skills and Training Rank (where 1 is most deprived)',\n",
       "       'Health Deprivation and Disability Rank (where 1 is most deprived)',\n",
       "       'Crime Rank (where 1 is most deprived)',\n",
       "       'Barriers to Housing and Services Rank (where 1 is most deprived)',\n",
       "       'Living Environment Rank (where 1 is most deprived)',\n",
       "       'Burglaries amount', 'Education locations', 'Emergency locations',\n",
       "       'Entertainment locations', 'Food locations', 'Leisure locations',\n",
       "       'Parking locations', 'Shopping locations', 'Public transport locations',\n",
       "       'Dwelling type|Flat, maisonette or apartment (%)',\n",
       "       'Ethnic Group|Asian/Asian British (%)', 'Ethnic Group|BAME (%)',\n",
       "       'Ethnic Group|Black/African/Caribbean/Black British (%)',\n",
       "       'Ethnic Group|Mixed/multiple ethnic groups (%)',\n",
       "       'Ethnic Group|Other ethnic group (%)', 'Ethnic Group|White (%)',\n",
       "       'Household Composition|% Couple household with dependent children',\n",
       "       'Household Composition|% Couple household without dependent children',\n",
       "       'Household Composition|% Lone parent household',\n",
       "       'Household Composition|% One person household',\n",
       "       'Household Composition|% Other multi person household',\n",
       "       'Households|All households', 'Mid-year Population Estimates|Aged 0-15',\n",
       "       'Mid-year Population Estimates|Aged 16-29',\n",
       "       'Mid-year Population Estimates|Aged 30-44',\n",
       "       'Mid-year Population Estimates|Aged 45-64',\n",
       "       'Mid-year Population Estimates|Aged 65+',\n",
       "       'Mid-year Population Estimates|All Ages',\n",
       "       'Mid-year Population Estimates|Working-age',\n",
       "       'Tenure|Owned outright (%)', 'Tenure|Owned with a mortgage or loan (%)',\n",
       "       'Tenure|Private rented (%)', 'Tenure|Social rented (%)',\n",
       "       'Car or van availability|1 car or van in household (%)',\n",
       "       'Car or van availability|2 cars or vans in household (%)',\n",
       "       'Car or van availability|3 cars or vans in household (%)',\n",
       "       'Car or van availability|4 or more cars or vans in household (%)',\n",
       "       'Car or van availability|Cars per household',\n",
       "       'Car or van availability|No cars or vans in household (%)',\n",
       "       'Public Transport Accessibility Levels|% 0-1 (poor access)|Level3_65',\n",
       "       'Public Transport Accessibility Levels|% 2-3 (average access)|Level3_66',\n",
       "       'Public Transport Accessibility Levels|% 4-6 (good access)|Level3_67',\n",
       "       'Public Transport Accessibility Levels|Average Score|Level3_64',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|0',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|1a',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|1b',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|2',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|3',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|4',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|5',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|6a',\n",
       "       'Public Transport Accessibility Levels|Number of people in each PTAL level:|6b',\n",
       "       'geometry'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_parquet(\"../merged_data.parquet\")\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d9a54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from calendar import month_name\n",
    "from typing import Any, Dict, List, Sequence, Tuple   # ← 2️⃣ add this\n",
    "\n",
    "def df_to_tensor_dict(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    lsoa_col: str = \"LSOA code (2021)\",\n",
    "    occ_col:  str | None = None,\n",
    "    static_cols: Sequence[str] | None = None,\n",
    "    dynamic_cols: Sequence[str] | None = None,\n",
    "    date_col: str = \"date\",\n",
    "    target_col: str = \"Burglaries amount\",\n",
    "    drop_cols: tuple[str, ...] = (\"geometry\",)\n",
    ") -> dict[str, np.ndarray]:\n",
    "\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"Empty slice passed to df_to_tensor_dict\")\n",
    "\n",
    "    # ── 1.  categorical indices ───────────────────────────────────────\n",
    "    ward_idx = df[lsoa_col].astype(\"category\").cat.codes.to_numpy(np.int64)\n",
    "    occupation_idx = (\n",
    "        np.zeros(n, dtype=np.int64) if occ_col is None or occ_col not in df\n",
    "        else df[occ_col].astype(\"category\").cat.codes.to_numpy(np.int64)\n",
    "    )\n",
    "\n",
    "    # ── 2.  choose numeric columns ────────────────────────────────────\n",
    "    numeric_cols = [c for c in df.select_dtypes(\"number\").columns if c not in drop_cols]\n",
    "\n",
    "    if static_cols is None or dynamic_cols is None:\n",
    "        varying = df[numeric_cols].nunique(dropna=False) > 1\n",
    "        inferred_dyn  = [c for c, v in varying.items() if v]\n",
    "        inferred_stat = [c for c in numeric_cols if c not in inferred_dyn]\n",
    "        dynamic_cols  = inferred_dyn  if dynamic_cols is None else list(dynamic_cols)\n",
    "        static_cols   = inferred_stat if static_cols  is None else list(static_cols)\n",
    "\n",
    "    X_static  = df[static_cols ].to_numpy(np.float32) if static_cols  else np.zeros((n, 0), np.float32)\n",
    "    X_dynamic = df[dynamic_cols].to_numpy(np.float32) if dynamic_cols else np.zeros((n, 0), np.float32)\n",
    "\n",
    "    # ── 3.  temporal features ─────────────────────────────────────────\n",
    "    months       = pd.to_datetime(df[date_col]).dt.month.to_numpy()         # 1-12\n",
    "    month_angle  = 2 * np.pi * (months - 1) / 12\n",
    "    month_sin    = np.sin(month_angle).reshape(-1, 1)                       # (n, 1)\n",
    "    month_cos    = np.cos(month_angle).reshape(-1, 1)                       # (n, 1)\n",
    "\n",
    "    # one-hot: shape (n, 12)\n",
    "    month_1h = np.eye(12, dtype=np.float32)[months - 1]\n",
    "\n",
    "    time_trend = np.arange(n, dtype=np.float32).reshape(-1, 1)\n",
    "    time_norm  = (time_trend - time_trend.mean()) / time_trend.std()\n",
    "\n",
    "    X_temporal = np.concatenate([month_sin, month_cos, time_norm], axis=1)  # (n, 3)\n",
    "\n",
    "    # ── 4.  map to model key names ─────────────────────────────────────\n",
    "    return {\n",
    "        \"occupation_idx\": torch.as_tensor(occupation_idx, dtype=torch.long),\n",
    "        \"ward_idx\":       torch.as_tensor(ward_idx,       dtype=torch.long),\n",
    "        \"X_static\":       torch.as_tensor(X_static,       dtype=torch.float32),\n",
    "        \"X_dynamic\":      torch.as_tensor(X_dynamic,      dtype=torch.float32),\n",
    "        \"X_seasonal\":     torch.as_tensor(month_1h,       dtype=torch.float32),  # ← fixed\n",
    "        \"X_time_trend\":   torch.as_tensor(time_trend,     dtype=torch.float32),  # raw trend\n",
    "        \"X_temporal\":     torch.as_tensor(X_temporal,     dtype=torch.float32),  # compact 3-col\n",
    "        \"X_spatial\":      torch.as_tensor(X_static,       dtype=torch.float32),  # alias for model\n",
    "        \"y\":              torch.as_tensor(df[target_col].to_numpy(),\n",
    "                                          dtype=torch.float32),\n",
    "        # dummy scalers so train_model never raises KeyError\n",
    "        \"means\": torch.zeros(1, dtype=torch.float32),\n",
    "        \"stds\":  torch.ones(1,  dtype=torch.float32),\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# rolling-origin CV for a DataFrame\n",
    "# --------------------------------------------------------------------\n",
    "def rolling_origin_cv(\n",
    "        \n",
    "        df: pd.DataFrame,\n",
    "        model_function,\n",
    "        inx_to_occupation_map: Dict[int, str],\n",
    "        *,\n",
    "        # ───────── parameters that control the rolling window ──────────\n",
    "        date_col: str = \"date\",\n",
    "        occ_col: str | None = None,          # ← NEW\n",
    "        static_cols: Sequence[str] | None = None,  # ← NEW (optional)\n",
    "        dynamic_cols: Sequence[str] | None = None, # ← NEW (optional)\n",
    "        initial_train_size: int | float | str | None = None,\n",
    "        horizon: int = 1,\n",
    "        step_size: int = 1,\n",
    "        expanding_window: bool = True,\n",
    "        max_splits: int | None = None,\n",
    "        # ───────── parameters forwarded to the learner ─────────────────\n",
    "        guide_type: str = \"diag\",\n",
    "        guide_rank: int = 10,\n",
    "        lr: float = 1e-3,\n",
    "        elbo_type: str = \"trace\",\n",
    "        renyi_alpha: float = 0.5,\n",
    "        num_particles: int = 1,\n",
    "        training_steps: int = 500,\n",
    "        testing_steps: int = 1000\n",
    ") -> Tuple[Dict[str, List[float]],        # aggregated metrics\n",
    "           List[Tuple[Dict[str, float],   # per-fold artefacts\n",
    "                     Any, Any, Any]]]:\n",
    "\n",
    "    if static_cols is None or dynamic_cols is None:\n",
    "        # use the *entire* data set to decide what varies\n",
    "        numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "        varying = df[numeric_cols].nunique(dropna=False) > 1\n",
    "        dynamic_cols = [c for c, v in varying.items() if v] \\\n",
    "                       if dynamic_cols is None else list(dynamic_cols)\n",
    "        static_cols  = [c for c in numeric_cols if c not in dynamic_cols] \\\n",
    "                       if static_cols  is None else list(static_cols)\n",
    "\n",
    "    # 1 ── make sure the frame is in chronological order\n",
    "    df = df.sort_values(date_col).reset_index(drop=True)\n",
    "    n_obs = len(df)\n",
    "    if n_obs < 3:\n",
    "        raise ValueError(\"Need at least three rows for cross-validation.\")\n",
    "\n",
    "    # 2 ── resolve initial_train_size\n",
    "    if initial_train_size is None:\n",
    "        initial_train_size = int(np.floor(0.7 * n_obs))\n",
    "    elif isinstance(initial_train_size, str) and initial_train_size.endswith('%'):\n",
    "        pct = float(initial_train_size.rstrip('%')) / 100.0\n",
    "        initial_train_size = int(np.floor(pct * n_obs))\n",
    "    elif isinstance(initial_train_size, float) and 0 < initial_train_size < 1:\n",
    "        initial_train_size = int(np.floor(initial_train_size * n_obs))\n",
    "\n",
    "    if initial_train_size < 1 or initial_train_size >= n_obs:\n",
    "        raise ValueError(\"initial_train_size must be between 1 and len(df)-1\")\n",
    "\n",
    "    # 3 ── bookkeeping containers\n",
    "    train_start, train_end = 0, initial_train_size\n",
    "    split_idx = 0\n",
    "    per_fold_vals: Dict[str, List[float]] = {\"rmse\": [], \"mae\": [], \"crps\": []}\n",
    "    fold_results: List[Tuple[Dict[str, float], Any, Any, Any]] = []\n",
    "\n",
    "    # 4 ── main walk-forward loop\n",
    "    while train_end + horizon <= n_obs and (max_splits is None or split_idx < max_splits):\n",
    "\n",
    "        # slice DataFrame with iloc, then convert to dict of arrays\n",
    "        train_df = df.iloc[train_start:train_end].reset_index(drop=True)\n",
    "        test_df  = df.iloc[train_end:train_end + horizon].reset_index(drop=True)\n",
    "\n",
    "        train_slice = df_to_tensor_dict(\n",
    "    train_df,\n",
    "    occ_col      = occ_col,\n",
    "    static_cols  = static_cols,\n",
    "    dynamic_cols = dynamic_cols,\n",
    "    date_col     = date_col\n",
    " \n",
    ")\n",
    "        test_slice  = df_to_tensor_dict(\n",
    "    test_df,\n",
    "    occ_col      = occ_col,\n",
    "    static_cols  = static_cols,\n",
    "    dynamic_cols = dynamic_cols,\n",
    "    date_col     = date_col\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "        metrics, svi, guide, tester = train_and_evaluate_model(\n",
    "            training_data          = train_slice,\n",
    "            testing_data           = test_slice,\n",
    "            model_function         = model_function,\n",
    "            inx_to_occupation_map  = inx_to_occupation_map,\n",
    "            guide_type             = guide_type,\n",
    "            guide_rank             = guide_rank,\n",
    "            lr                     = lr,\n",
    "            elbo_type              = elbo_type,\n",
    "            renyi_alpha            = renyi_alpha,\n",
    "            num_particles          = num_particles,\n",
    "            training_steps         = training_steps,\n",
    "            testing_steps          = testing_steps\n",
    "        )\n",
    "\n",
    "        for key in per_fold_vals:\n",
    "            per_fold_vals[key].append(metrics[key])\n",
    "\n",
    "        fold_results.append((metrics, svi, guide, tester))\n",
    "\n",
    "        # shift origin\n",
    "        split_idx += 1\n",
    "        if expanding_window:\n",
    "            train_end += step_size\n",
    "        else:\n",
    "            train_start += step_size\n",
    "            train_end   += step_size\n",
    "\n",
    "    # 5 ── sanity check: at least one split produced?\n",
    "    if split_idx == 0:\n",
    "        raise ValueError(\n",
    "            f\"No CV folds were created. \"\n",
    "            f\"Check initial_train_size={initial_train_size}, horizon={horizon}, \"\n",
    "            f\"step_size={step_size}, len(df)={n_obs}.\"\n",
    "        )\n",
    "\n",
    "    # 6 ── aggregate statistics (append mean as last element)\n",
    "    for key, vals in per_fold_vals.items():\n",
    "        per_fold_vals[key].append(float(np.mean(vals)))\n",
    "\n",
    "    return per_fold_vals, fold_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1834953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_metrics, folds = rolling_origin_cv(\n",
    "#         df                     = data,\n",
    "#         model_function         = burglary_model,\n",
    "#         inx_to_occupation_map  = occupation_mappings[1],\n",
    "#         horizon                = 7,\n",
    "#         step_size              = 10,\n",
    "#         initial_train_size     = \"60%\",\n",
    "#         expanding_window       = True,\n",
    "#         occ_col                = None          # or \"your_occupation_column\"\n",
    "# )\n",
    "\n",
    "# print(\"Per-fold RMSE:\", cv_metrics[\"rmse\"][:-1])\n",
    "# print(\"Overall  RMSE:\", cv_metrics[\"rmse\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_n_time_splits(df: pd.DataFrame,\n",
    "                       time_col: str = \"time_s\",\n",
    "                       n_splits: int = 12):\n",
    "    results = []\n",
    "    results.extend(\n",
    "        (\n",
    "            df.query(f\"{time_col} < {time_vallue}\").copy(),\n",
    "            df.query(f\"{time_col} == {time_vallue}\").copy(),\n",
    "        )\n",
    "        for time_vallue in df.sort_values(time_col)[time_col]\n",
    "        .unique()\n",
    "        .tolist()[-n_splits:]\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SVI: 100%|██████████| 500/500 [00:18<00:00, 26.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics for split: {'rmse': np.float32(2.804358), 'mae': np.float32(1.5840049), 'crps': 2.3544168628179043}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SVI:  60%|██████    | 301/500 [00:10<00:06, 29.51it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m train_data = prepare_model_data(split[\u001b[32m0\u001b[39m], *model_tuple[\u001b[32m1\u001b[39m:], device, ward_idx_map=ward_idx_map)\n\u001b[32m      8\u001b[39m test_data = prepare_model_data(split[\u001b[32m1\u001b[39m], *model_tuple[\u001b[32m1\u001b[39m:], device, train_data[\u001b[33m\"\u001b[39m\u001b[33mmeans\u001b[39m\u001b[33m\"\u001b[39m], train_data[\u001b[33m\"\u001b[39m\u001b[33mstds\u001b[39m\u001b[33m\"\u001b[39m], ward_idx_map)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m evaluation_metrics, _, _, _ = \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mburglary_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moccupation_mappings\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation metrics for split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation_metrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chekm\\Jupiter\\ARWCASP\\model\\pipeline.py:29\u001b[39m, in \u001b[36mtrain_and_evaluate_model\u001b[39m\u001b[34m(training_data, testing_data, model_function, inx_to_occupation_map, guide_type, guide_rank, lr, elbo_type, renyi_alpha, num_particles, training_steps, testing_steps)\u001b[39m\n\u001b[32m     19\u001b[39m pyro.clear_param_store()\n\u001b[32m     21\u001b[39m svi = create_learner(model_function,\n\u001b[32m     22\u001b[39m                      guide_type=guide_type,\n\u001b[32m     23\u001b[39m                      guide_rank=guide_rank,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m                      renyi_alpha=renyi_alpha,\n\u001b[32m     27\u001b[39m                      num_particles=num_particles)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m _ = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m prediction_tester = PredictionTester(\n\u001b[32m     31\u001b[39m     testing_data, burglary_model, svi.guide, inx_to_occupation_map)\n\u001b[32m     32\u001b[39m prediction_tester.predict(testing_steps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Chekm\\Jupiter\\ARWCASP\\model\\training\\training.py:178\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(data, svi, num_steps)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_steps), desc=\u001b[33m\"\u001b[39m\u001b[33mTraining SVI\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     loss = \u001b[43msvi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43moccupation_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mward_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_static\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_dynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_seasonal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_time_trend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_temporal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_spatial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m     losses.append(loss)\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m svi, losses, data[\u001b[33m\"\u001b[39m\u001b[33mmeans\u001b[39m\u001b[33m\"\u001b[39m], data[\u001b[33m\"\u001b[39m\u001b[33mstds\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MiniConda\\envs\\py311\\Lib\\site-packages\\pyro\\infer\\svi.py:145\u001b[39m, in \u001b[36mSVI.step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m poutine.trace(param_only=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m params = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m    148\u001b[39m     site[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m].unconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture.trace.nodes.values()\n\u001b[32m    149\u001b[39m )\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MiniConda\\envs\\py311\\Lib\\site-packages\\pyro\\infer\\trace_elbo.py:140\u001b[39m, in \u001b[36mTrace_ELBO.loss_and_grads\u001b[39m\u001b[34m(self, model, guide, *args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m loss = \u001b[32m0.0\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# grab a trace from the generator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide_trace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_traces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_particle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurrogate_loss_particle\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_differentiable_loss_particle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide_trace\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_particle\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_particles\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MiniConda\\envs\\py311\\Lib\\site-packages\\pyro\\infer\\elbo.py:237\u001b[39m, in \u001b[36mELBO._get_traces\u001b[39m\u001b[34m(self, model, guide, args, kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_particles):\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MiniConda\\envs\\py311\\Lib\\site-packages\\pyro\\infer\\trace_elbo.py:57\u001b[39m, in \u001b[36mTrace_ELBO._get_trace\u001b[39m\u001b[34m(self, model, guide, args, kwargs)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, args, kwargs):\n\u001b[32m     53\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03m    Returns a single trace from the guide, and the model that is run\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03m    against it.\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     model_trace, guide_trace = \u001b[43mget_importance_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_plate_nesting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[32m     61\u001b[39m         check_if_enumerated(guide_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MiniConda\\envs\\py311\\Lib\\site-packages\\pyro\\infer\\enum.py:76\u001b[39m, in \u001b[36mget_importance_trace\u001b[39m\u001b[34m(graph_type, max_plate_nesting, model, guide, args, kwargs, detach)\u001b[39m\n\u001b[32m     73\u001b[39m model_trace = prune_subsample_sites(model_trace)\n\u001b[32m     75\u001b[39m model_trace.compute_log_prob()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[43mguide_trace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_score_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m model_trace.nodes.values():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MiniConda\\envs\\py311\\Lib\\site-packages\\pyro\\poutine\\trace_struct.py:324\u001b[39m, in \u001b[36mTrace.compute_score_parts\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_validation_enabled():\n\u001b[32m    321\u001b[39m     warn_if_nan(\n\u001b[32m    322\u001b[39m         site[\u001b[33m\"\u001b[39m\u001b[33mlog_prob_sum\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mlog_prob_sum at site \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m.format(name)\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     \u001b[43mwarn_if_inf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m        \u001b[49m\u001b[43msite\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlog_prob_sum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlog_prob_sum at site \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_neginf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MiniConda\\envs\\py311\\Lib\\site-packages\\pyro\\util.py:130\u001b[39m, in \u001b[36mwarn_if_inf\u001b[39m\u001b[34m(value, msg, allow_posinf, allow_neginf, filename, lineno)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(value) \u001b[38;5;129;01mand\u001b[39;00m value.requires_grad:\n\u001b[32m    118\u001b[39m     value.register_hook(\n\u001b[32m    119\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: warn_if_inf(\n\u001b[32m    120\u001b[39m             x,\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m         )\n\u001b[32m    127\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_posinf) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     value == math.inf\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, numbers.Number)\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (value == math.inf).any()\n\u001b[32m    133\u001b[39m ):\n\u001b[32m    134\u001b[39m     warnings.warn_explicit(\n\u001b[32m    135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEncountered +inf\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[33m\"\u001b[39m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m + msg \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    136\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    137\u001b[39m         filename,\n\u001b[32m    138\u001b[39m         lineno,\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_neginf) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    141\u001b[39m     value == -math.inf\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, numbers.Number)\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (value == -math.inf).any()\n\u001b[32m    144\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\MiniConda\\envs\\py311\\Lib\\site-packages\\torch\\utils\\_device.py:104\u001b[39m, in \u001b[36mDeviceContext.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs.get(\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    103\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Any, Dict\n",
    "from utils.utils import setup_reproducibility\n",
    "\n",
    "def cross_validate_time_splits(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str,\n",
    "    n_splits: int,\n",
    "    model_tuple: Any,\n",
    "    burglary_model: Any,\n",
    "    occupation_map: Dict[int, str],\n",
    "    device,\n",
    "    ward_idx_map: Dict[Any, Any],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform rolling‐window cross‐validation over the last `n_splits` time points.\n",
    "    For each split, train on the immediately preceding time_s value and test on that time_s.\n",
    "    Returns a DataFrame summarizing the evaluation metrics for each split.\n",
    "    \"\"\"\n",
    "    splits = last_n_time_splits(df, time_col=time_col, n_splits=n_splits)\n",
    "    records = []\n",
    "\n",
    "    for train_df, test_df in splits:\n",
    "        train_time = train_df[time_col].iloc[0]\n",
    "        test_time  = test_df[time_col].iloc[0]\n",
    "\n",
    "        # Prepare data\n",
    "        train_data = prepare_model_data(\n",
    "            train_df,\n",
    "            *model_tuple[1:],\n",
    "            device,\n",
    "            ward_idx_map=ward_idx_map\n",
    "        )\n",
    "        test_data = prepare_model_data(\n",
    "            test_df,\n",
    "            *model_tuple[1:],\n",
    "            device,\n",
    "            train_data[\"means\"],\n",
    "            train_data[\"stds\"],\n",
    "            ward_idx_map=ward_idx_map\n",
    "        )\n",
    "\n",
    "        # Train & evaluate\n",
    "        evaluation_metrics, _, _, _ = train_and_evaluate_model(\n",
    "            train_data,\n",
    "            test_data,\n",
    "            burglary_model,\n",
    "            occupation_map\n",
    "        )\n",
    "\n",
    "        # Record metrics along with split info\n",
    "        record = {\n",
    "            \"train_time\": train_time,\n",
    "            \"test_time\":  test_time,\n",
    "            **evaluation_metrics\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# 1) Set up reproducibility\n",
    "from utils.utils import setup_reproducibility\n",
    "device = setup_reproducibility(42)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2) Run cross‐validation\n",
    "cv_df = cross_validate_time_splits(\n",
    "    df=model_tuple[0],\n",
    "    time_col=\"time_s\",\n",
    "    n_splits=3,\n",
    "    model_tuple=model_tuple,\n",
    "    burglary_model=burglary_model,\n",
    "    occupation_map=occupation_mappings[1],\n",
    "    device=device,\n",
    "    ward_idx_map=ward_idx_map\n",
    ")\n",
    "\n",
    "print(cv_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901eeef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
