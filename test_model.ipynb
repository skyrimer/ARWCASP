{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_parquet(\"merged_data.parquet\").rename(columns={\n",
    "        'LSOA code (2021)': 'LSOA code',\n",
    "        'date': 'period',\n",
    "        'Index of Multiple Deprivation (IMD) Rank (where 1 is most deprived)': 'imd_rank',\n",
    "        'Burglaries amount': 'burglaries'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a73eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def downcast_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1. downcast integers\n",
    "    int_cols = df.select_dtypes(include=['int64']).columns\n",
    "    df[int_cols] = (\n",
    "        df[int_cols]\n",
    "        .apply(pd.to_numeric, downcast='integer')\n",
    "    )\n",
    "    \n",
    "    # 2. downcast floats\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    df[float_cols] = (\n",
    "        df[float_cols]\n",
    "        .apply(pd.to_numeric, downcast='float')\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage:\n",
    "gdf[\"LSOA code\"] = gdf[\"LSOA code\"].astype(\"category\")\n",
    "\n",
    "gdf = downcast_numeric(gdf)\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e603940",
   "metadata": {},
   "outputs": [],
   "source": [
    "static = {\n",
    "   \"Education locations\", \"Emergency locations\", \"Entertainment locations\",\n",
    "   \"Food locations\", \"Leisure locations\", \"Parking locations\", \"Shopping locations\",\n",
    "   \"Public transport locations\", \"Dwelling type|Flat, maisonette or apartment (%)\",\n",
    "   \"Ethnic Group|Asian/Asian British (%)\", \"Ethnic Group|BAME (%)\", \"Ethnic Group|Black/African/Caribbean/Black British (%)\",\n",
    "   \"Ethnic Group|Mixed/multiple ethnic groups (%)\", \"Ethnic Group|Other ethnic group (%)\",\n",
    "   \"Ethnic Group|White (%)\", \"Household Composition|% Couple household with dependent children\",\n",
    "   \"Household Composition|% Couple household without dependent children\",\n",
    "   r\"Household Composition|% Lone parent household\",\n",
    "    \"Household Composition|% One person household\",\n",
    "    \"Household Composition|% Other multi person household\",\n",
    "    \"Households|All households\", \"Tenure|Owned outright (%)\",\n",
    "    \"Tenure|Owned with a mortgage or loan (%)\", \"Tenure|Private rented (%)\",\n",
    "    \"Tenure|Social rented (%)\", \"Car or van availability|1 car or van in household (%)\",\n",
    "    \"Car or van availability|2 cars or vans in household (%)\",\n",
    "    \"Car or van availability|3 cars or vans in household (%)\",\n",
    "    \"Car or van availability|4 or more cars or vans in household (%)\",\n",
    "    \"Car or van availability|Cars per household\",\n",
    "    \"Car or van availability|No cars or vans in household (%)\",\n",
    "    \"Public Transport Accessibility Levels|% 0-1 (poor access)|Level3_65\",       \n",
    "    \"Public Transport Accessibility Levels|% 2-3 (average access)|Level3_66\",       \n",
    "    \"Public Transport Accessibility Levels|% 4-6 (good access)|Level3_67\",       \n",
    "    \"Public Transport Accessibility Levels|Average Score|Level3_64\",       \n",
    "    \"Public Transport Accessibility Levels|Number of people in each PTAL level:|0\",       \n",
    "    \"Public Transport Accessibility Levels|Number of people in each PTAL level:|1a\",       \n",
    "    \"Public Transport Accessibility Levels|Number of people in each PTAL level:|1b\",       \n",
    "    \"Public Transport Accessibility Levels|Number of people in each PTAL level:|2\",       \n",
    "    \"Public Transport Accessibility Levels|Number of people in each PTAL level:|3\",       \n",
    "    \"Public Transport Accessibility Levels|Number of people in each PTAL level:|4\",       \n",
    "    \"Public Transport Accessibility Levels|Number of people in each PTAL level:|5\",       \n",
    "    \"Public Transport Accessibility Levels|Number of people in each PTAL level:|6a\",       \n",
    "    \"Public Transport Accessibility Levels|Number of people in each PTAL level:|6b\",\n",
    "}\n",
    "\n",
    "dynamic = set(gdf.drop(columns=[\"period\", \"geometry\", \"LSOA code\", \"burglaries\"]).columns) - set(static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69e0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = gdf['period'].min()\n",
    "# convert each period to the number of months since t0\n",
    "time_idx = (\n",
    "    (gdf['period'].dt.year  - t0.year) * 12\n",
    "  + (gdf['period'].dt.month - t0.month)\n",
    ")\n",
    "# center & scale\n",
    "gdf['time_s'] = (time_idx - time_idx.mean()) / time_idx.std()\n",
    "gdf['month_sin'] = np.sin(2*np.pi*gdf['period'].dt.month/12)\n",
    "gdf['month_cos'] = np.cos(2*np.pi*gdf['period'].dt.month/12)\n",
    "# Temporal trend\n",
    "gdf['prev_month']  = gdf.groupby('LSOA code', observed=True)['burglaries'].shift(1)\n",
    "gdf['prev_year'] = gdf.groupby('LSOA code', observed=True)['burglaries'].shift(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def create_length_weighted_adjacency(gdf: gpd.GeoDataFrame):\n",
    "    \"\"\"\n",
    "    From a GeoDataFrame with columns ['LSOA code','geometry'],\n",
    "    build:\n",
    "      - W: an N×N sparse matrix where W[i,j] = length of shared border between i and j\n",
    "      - D: the diagonal degree‐matrix with D[i,i] = sum_j W[i,j]\n",
    "      - idx_map: mapping LSOA code -> integer index i in [0..N-1]\n",
    "    \"\"\"\n",
    "    # 1) Extract one geometry per LSOA and fix geometry validity\n",
    "    geo = (\n",
    "        gdf[['LSOA code','geometry']]\n",
    "        .drop_duplicates('LSOA code')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    geo['geometry'] = geo['geometry'].buffer(0)\n",
    "\n",
    "    # 2) Build index mapping\n",
    "    codes   = geo['LSOA code'].tolist()\n",
    "    idx_map = {code: idx for idx, code in enumerate(codes)}\n",
    "    N       = len(codes)\n",
    "\n",
    "    rows, cols, data = [], [], []\n",
    "\n",
    "    # 3) Loop over each LSOA and its touching neighbours\n",
    "    for i, row in geo.iterrows():\n",
    "        code_i, geom_i = row['LSOA code'], row['geometry']\n",
    "        # find those that touch\n",
    "        touching = geo[geo.geometry.touches(geom_i)]\n",
    "        for _, nbr in touching.iterrows():\n",
    "            j = idx_map[nbr['LSOA code']]\n",
    "            # only handle each pair once (i<j)\n",
    "            if i >= j:\n",
    "                continue\n",
    "            geom_j = nbr['geometry']\n",
    "            # compute shared boundary length\n",
    "            shared = geom_i.intersection(geom_j)\n",
    "            length = shared.length\n",
    "            if length > 0:\n",
    "                # add both directions\n",
    "                rows += [i, j]\n",
    "                cols += [j, i]\n",
    "                data += [length, length]\n",
    "\n",
    "    # 4) Build sparse W and degree‐matrix D\n",
    "    W = csr_matrix((data, (rows, cols)), shape=(N, N), dtype=float)\n",
    "    deg = np.array(W.sum(axis=1)).ravel()\n",
    "    D = csr_matrix((deg, (np.arange(N), np.arange(N))), shape=(N, N))\n",
    "\n",
    "    return W, D, idx_map\n",
    "\n",
    "\n",
    "W, D, idx_map = create_length_weighted_adjacency(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64bf6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = gdf.dropna(subset=[\"prev_month\", \"prev_year\"]).copy()\n",
    "matrix['lsoa_idx'], cats = pd.factorize(matrix['LSOA code'])\n",
    "matrix = matrix.drop(columns=[\"geometry\", \"period\", \"LSOA code\"])\n",
    "# TODO: proper scaling of all parameters if needed\n",
    "# for col in matrix.drop(columns=[\"lsoa_idx\"]).columns:\n",
    "# # for col in matrix.drop(columns=[\"lsoa_idx\", \"burglaries\", \"month_sin\", \"month_cos\", \"prev_year\", \"prev_month\"]).columns:\n",
    "#     matrix[col] = np.log1p(matrix[col])\n",
    "#     matrix[col] = (matrix[col] - matrix[col].mean())/matrix[col].std()\n",
    "\n",
    "matrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonal = {\n",
    "    \"month_sin\", \"month_cos\"\n",
    "}\n",
    "time_trend = {\n",
    "    \"time_s\"\n",
    "}\n",
    "temporal = {\n",
    "    \"prev_month\", \"prev_year\"\n",
    "}\n",
    "set(matrix.columns) - static - dynamic - seasonal - time_trend - temporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c86b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b3f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.min().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243d16e",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6ab771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # Cell 1: Imports & Data Preparation\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# import numpy as np\n",
    "# import jax.numpy as jnp\n",
    "# from jax import random, device_put\n",
    "# import numpyro\n",
    "# import numpyro.distributions as dist\n",
    "# from numpyro.infer import SVI, Trace_ELBO, Predictive\n",
    "# from numpyro.infer.autoguide import AutoNormal\n",
    "# from numpyro.infer.initialization import init_to_feasible\n",
    "# from numpyro.optim import Adam\n",
    "\n",
    "# # Assume you’ve already got in your namespace:\n",
    "# #   • matrix        : pandas.DataFrame with columns\n",
    "# #       'lsoa_idx','burglaries',\n",
    "# #       static_cols (e.g. ['static_1','static_2','static_3']),\n",
    "# #       dynamic_cols (e.g. ['dynamic_1','dynamic_2','dynamic_3']),\n",
    "# #       'time_s','month_sin','month_cos','prev_month'\n",
    "# #   • static_cols   : list of static column names\n",
    "# #   • dynamic_cols  : list of dynamic column names\n",
    "# #   • W             : (N×N) NumPy array of shared-border weights\n",
    "\n",
    "# static_cols = list(static)\n",
    "# dynamic_cols = list(dynamic)\n",
    "# # 1) Convert adjacency to JAX & build degree matrix\n",
    "# W_jax = device_put(jnp.array(W.toarray()))                     # [N×N]\n",
    "# D_jax = device_put(jnp.diag(D.toarray()))                      # [N×N]\n",
    "\n",
    "# # 2) Static matrix [N × P_static]\n",
    "# static_df = (\n",
    "#     matrix[['lsoa_idx'] + static_cols]\n",
    "#     .drop_duplicates('lsoa_idx')\n",
    "#     .sort_values('lsoa_idx')\n",
    "# )\n",
    "# static_matrix = device_put(\n",
    "#     jnp.array(static_df[static_cols].values)\n",
    "# )\n",
    "\n",
    "# # 3) Dynamic matrix [n_obs × P_dynamic]\n",
    "# dynamic_matrix = device_put(\n",
    "#     jnp.array(matrix[dynamic_cols].values)\n",
    "# )\n",
    "\n",
    "# # 4) Vectorize other columns\n",
    "# vecs = {\n",
    "#     'lsoa_idx': device_put(jnp.array(matrix['lsoa_idx'].values)),   # [n_obs]\n",
    "#     'counts':   device_put(jnp.array(matrix['burglaries'].values)), # [n_obs]\n",
    "#     'time_s':   device_put(jnp.array(matrix['time_s'].values)),     # [n_obs]\n",
    "#     'sin':      device_put(jnp.array(matrix['month_sin'].values)),  # [n_obs]\n",
    "#     'cos':      device_put(jnp.array(matrix['month_cos'].values)),  # [n_obs]\n",
    "#     'lag1':     device_put(jnp.array(matrix['prev_month'].values)),  # [n_obs]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3993e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # Cell 2: Model Definition & SVI Function\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "# def hierarchical_nb_full(\n",
    "#     lsoa_idx, counts, n_lsoas,\n",
    "#     static_matrix, n_static,\n",
    "#     time_s, sin, cos, lag1,\n",
    "#     dynamic_matrix, n_dynamic,\n",
    "#     W, D\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Hierarchical NB with:\n",
    "#       • static covariates\n",
    "#       • continuous trend & seasonality\n",
    "#       • one‐month self‐lag\n",
    "#       • dynamic covariates\n",
    "#       • CAR spatial random effect\n",
    "#     \"\"\"\n",
    "#     # 1) Static effects\n",
    "#     sigma_s    = numpyro.sample('sigma_static', dist.HalfNormal(1.0))\n",
    "#     beta_s     = numpyro.sample(\n",
    "#         'beta_static',\n",
    "#         dist.Normal(0., sigma_s).expand([n_static])\n",
    "#     )\n",
    "#     static_eff = (static_matrix[lsoa_idx] * beta_s).sum(-1)\n",
    "\n",
    "#     # 2) Time trend\n",
    "#     beta_time = numpyro.sample('beta_time', dist.Normal(0., 0.25))\n",
    "#     time_eff  = beta_time * time_s\n",
    "\n",
    "#     # 3) Seasonality\n",
    "#     beta_sin  = numpyro.sample('beta_sin', dist.Normal(0., 0.25))\n",
    "#     beta_cos  = numpyro.sample('beta_cos', dist.Normal(0., 0.25))\n",
    "#     seas_eff  = beta_sin * sin + beta_cos * cos\n",
    "\n",
    "#     # 4) Self‐lag\n",
    "#     beta_lag1 = numpyro.sample('beta_lag1', dist.Normal(0., 0.25))\n",
    "#     lag_eff   = beta_lag1 * lag1\n",
    "\n",
    "#     # 5) Dynamic covariates\n",
    "#     tau_d      = numpyro.sample('tau_dyn',  dist.HalfNormal(0.5))\n",
    "#     beta_d     = numpyro.sample(\n",
    "#         'beta_dyn',\n",
    "#         dist.Normal(0., tau_d).expand([n_dynamic])\n",
    "#     )\n",
    "#     dyn_eff    = (dynamic_matrix * beta_d).sum(-1)\n",
    "\n",
    "#     # 6) Spatial CAR random effect\n",
    "#     tau_u      = numpyro.sample('tau_u', dist.HalfNormal(1.0))\n",
    "#     rho        = numpyro.sample('rho',   dist.Uniform(0., 1.0))\n",
    "#     Q          = tau_u * (D - rho * W)\n",
    "#     u_raw      = numpyro.sample(\n",
    "#         'u_raw',\n",
    "#         dist.MultivariateNormal(jnp.zeros(n_lsoas), precision_matrix=Q)\n",
    "#     )\n",
    "#     u          = u_raw - jnp.mean(u_raw)\n",
    "#     spatial_re = u[lsoa_idx]\n",
    "\n",
    "#     # 7) Intercept & overdispersion\n",
    "#     alpha = numpyro.sample('alpha', dist.Normal(jnp.log(12.8), 0.25))\n",
    "#     phi   = numpyro.sample('phi',   dist.Gamma(2., 1.0))\n",
    "\n",
    "#     # 8) Linear predictor & likelihood\n",
    "#     log_mu = (alpha\n",
    "#             + static_eff\n",
    "#             + time_eff\n",
    "#             + seas_eff\n",
    "#             + lag_eff\n",
    "#             + dyn_eff\n",
    "#             + spatial_re)\n",
    "#     mu = jnp.exp(log_mu)\n",
    "\n",
    "#     numpyro.sample('y_obs',\n",
    "#                    dist.NegativeBinomial2(phi, mu),\n",
    "#                    obs=counts)\n",
    "\n",
    "\n",
    "# def run_svi_full(\n",
    "#     static_matrix, dynamic_matrix, W, D, vecs,\n",
    "#     n_lsoas, n_static, n_dynamic,\n",
    "#     num_steps=3000, lr=5e-5, seed=0\n",
    "# ):\n",
    "#     # 1) Build data dict\n",
    "#     data = {\n",
    "#         'n_lsoas':        n_lsoas,\n",
    "#         'static_matrix':  static_matrix,\n",
    "#         'n_static':       n_static,\n",
    "#         'dynamic_matrix': dynamic_matrix,\n",
    "#         'n_dynamic':      n_dynamic,\n",
    "#         'W':              W,\n",
    "#         'D':              D,\n",
    "#         **vecs,\n",
    "#     }\n",
    "\n",
    "#     # 2) Prior predictive check\n",
    "#     print(\"Running prior predictive check…\")\n",
    "#     prior   = Predictive(hierarchical_nb_full, params=None, num_samples=200)\n",
    "#     pd_draws= prior(random.PRNGKey(seed), **data)['y_obs']\n",
    "#     lower   = np.percentile(pd_draws, 3,  axis=0)\n",
    "#     upper   = np.percentile(pd_draws, 97, axis=0)\n",
    "#     pd_cov  = ((data['counts'] >= lower) & (data['counts'] <= upper)).mean()\n",
    "#     print(f\"Prior coverage (94% CI): {pd_cov:.2%}\")\n",
    "\n",
    "#     # 3) SVI setup\n",
    "#     guide = AutoNormal(hierarchical_nb_full, init_loc_fn=init_to_feasible)\n",
    "#     svi   = SVI(hierarchical_nb_full, guide, Adam(lr), Trace_ELBO())\n",
    "#     rng   = random.PRNGKey(seed)\n",
    "#     state = svi.init(rng, **data)\n",
    "\n",
    "#     # 4) Optimization loop\n",
    "#     for i in range(num_steps):\n",
    "#         rng, _ = random.split(rng)\n",
    "#         state, loss = svi.update(state, **data)\n",
    "#         if i % 500 == 0:\n",
    "#             print(f\"Step {i:>4d} loss = {loss:.1f}\")\n",
    "\n",
    "#     # 5) Return fitted guide & params\n",
    "#     return svi.get_params(state), guide, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c964ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# # Cell 3: Run the SVI\n",
    "# # ──────────────────────────────────────────────────────────────────────────────\n",
    "# params, guide, data_dict = run_svi_full(\n",
    "#     static_matrix, dynamic_matrix, W_jax, D_jax, vecs,\n",
    "#     n_lsoas   = matrix['lsoa_idx'].nunique(),\n",
    "#     n_static  = len(static_cols),\n",
    "#     n_dynamic = len(dynamic_cols),\n",
    "#     num_steps = 500,\n",
    "#     lr        = 5e-5,\n",
    "#     seed      = 42\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98852c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pyro\n",
    "# import pyro.distributions as dist\n",
    "# from pyro.infer import SVI, Trace_ELBO, MCMC, NUTS\n",
    "# from pyro.optim import ClippedAdam\n",
    "# from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "# # 1) Select device\n",
    "# pyro.clear_param_store()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.set_default_device(device)\n",
    "# print(\"Using device:\", device)                        # should print \"cuda\" :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# # 2) Generate synthetic data on GPU\n",
    "# true_loc = 1.0\n",
    "# true_scale = 2.0\n",
    "# N = 5000\n",
    "# data = dist.Normal(true_loc, true_scale).sample([N]).to(device)\n",
    "\n",
    "# # 3) Define model with parameters on GPU\n",
    "# def model(data):\n",
    "#     # Prior on the mean\n",
    "#     mu = pyro.sample(\"mu\", dist.Normal(torch.tensor(0., device=device), torch.tensor(5., device=device)).to_event(0)) \n",
    "#     # Prior on the scale (must be positive)\n",
    "#     sigma = pyro.sample(\"sigma\", dist.HalfCauchy(torch.tensor(2., device=device)).to_event(0))\n",
    "#     with pyro.plate(\"data\", data.shape[0]):\n",
    "#         # Likelihood: observe data points\n",
    "#         pyro.sample(\"obs\", dist.Normal(mu, sigma), obs=data)\n",
    "\n",
    "# # 4) Define a simple AutoGuide on GPU\n",
    "# guide = AutoDiagonalNormal(model)\n",
    "\n",
    "# # 5) Set up SVI with a GPU-backed optimizer\n",
    "# svi = SVI(model, guide, ClippedAdam({\"lr\": 0.02}), loss=Trace_ELBO())\n",
    "\n",
    "# # 6) Run VI for a few hundred steps\n",
    "# num_steps = 1000\n",
    "# for step in range(num_steps):\n",
    "#     loss = svi.step(data)\n",
    "#     if step % 100 == 0:\n",
    "#         print(f\"Step {step} \\t ELBO loss = {loss:.1f}\")  # This loop should run on GPU :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# # 7) Draw posterior samples with NUTS on GPU\n",
    "# nuts_kernel = NUTS(model)\n",
    "# mcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=200)\n",
    "# mcmc.run(data)                                        # runs on GPU, provided all tensors are on cuda :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "# # 8) Examine posterior summaries\n",
    "# posterior = mcmc.get_samples()\n",
    "# print({k: v.mean().item() for k, v in posterior.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1442e2",
   "metadata": {},
   "source": [
    "# Pyro test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# assume matrix_df is your full DataFrame, with a Period‐like “period” column:\n",
    "#   columns: ['period','lsoa_idx','burglaries', *all your 64 features*, ...]\n",
    "\n",
    "# 3) split into training vs. forecast\n",
    "#    – matrix: all rows before the last month\n",
    "#    – next_month: only the last month’s rows\n",
    "matrix      = matrix[matrix['time_s'] <  matrix['time_s'].max()].copy()\n",
    "next_month  = matrix[matrix['time_s'] == matrix['time_s'].max()].copy()\n",
    "\n",
    "print(\"Training months:\", matrix['time_s'].min(), \"→\", matrix['time_s'].max())\n",
    "print(\"Forecast month:\", next_month['time_s'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import ClippedAdam\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 0) GPU + Pyro setup\n",
    "pyro.clear_param_store()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1) load_and_prepare now takes a DataFrame `df` already in memory\n",
    "def load_and_prepare(df):\n",
    "    # a) extract target & index\n",
    "    y = df[\"burglaries\"].astype(np.int32).values\n",
    "    idx = df[\"lsoa_idx\"].astype(np.int64).values\n",
    "\n",
    "    # b) build feature matrix from all cols except these two\n",
    "    feat_cols = [c for c in df.columns if c not in (\"burglaries\", \"lsoa_idx\")]\n",
    "    X = df[feat_cols].values.astype(np.float32)\n",
    "\n",
    "    # c) drop rows with any NaN/inf\n",
    "    mask = np.isfinite(X).all(axis=1)\n",
    "    X, y, idx = X[mask], y[mask], idx[mask]\n",
    "\n",
    "    # d) standardize features\n",
    "    means = X.mean(axis=0)\n",
    "    stds  = X.std(axis=0) + 1e-6\n",
    "    X = (X - means) / stds\n",
    "\n",
    "    # e) move to GPU tensors\n",
    "    return {\n",
    "        \"lsoa_idx\": torch.tensor(idx, dtype=torch.long,   device=device),\n",
    "        \"X\":        torch.tensor(X,   dtype=torch.float32,device=device),\n",
    "        \"y\":        torch.tensor(y,   dtype=torch.int32,  device=device),\n",
    "        \"n_lsoas\":  int(idx.max()) + 1,\n",
    "        \"n_attrs\":  X.shape[1],\n",
    "        \"feat_cols\": feat_cols,\n",
    "        \"means\": means,\n",
    "        \"stds\": stds,\n",
    "    }\n",
    "\n",
    "data = load_and_prepare(matrix)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2) Hierarchical Poisson model (unchanged)\n",
    "def burglary_model(lsoa_idx, X, y=None, use_subsample=True):\n",
    "    n_lsoas = data[\"n_lsoas\"]\n",
    "    n_attrs = data[\"n_attrs\"]\n",
    "\n",
    "    mu_a    = pyro.sample(\"mu_a\",    dist.Normal(0., 1.0))\n",
    "    sigma_a = pyro.sample(\"sigma_a\", dist.HalfNormal(1.0))\n",
    "\n",
    "    with pyro.plate(\"ls\", n_lsoas):\n",
    "        a = pyro.sample(\"a\", dist.Normal(mu_a, sigma_a))\n",
    "\n",
    "    b_attr = pyro.sample(\n",
    "        \"b_attr\",\n",
    "        dist.Normal(0., 1.0).expand([n_attrs]).to_event(1)\n",
    "    )\n",
    "\n",
    "    N = lsoa_idx.shape[0]\n",
    "\n",
    "    if use_subsample and y is not None:\n",
    "        with pyro.plate(\"data\", size=N, subsample_size=2048) as i:\n",
    "            eta = a[lsoa_idx[i]] + (X[i] * b_attr).sum(-1)\n",
    "            mu  = torch.exp(eta.clamp(-10, 10))\n",
    "            pyro.sample(\"obs\", dist.Poisson(mu), obs=y[i])\n",
    "    else:\n",
    "        with pyro.plate(\"data\", N):\n",
    "            eta = a[lsoa_idx] + (X * b_attr).sum(-1)\n",
    "            mu  = torch.exp(eta.clamp(-10, 10))\n",
    "            pyro.sample(\"obs\", dist.Poisson(mu), obs=y if y is not None else None)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Fit with SVI\n",
    "guide = AutoDiagonalNormal(burglary_model)\n",
    "svi   = SVI(burglary_model, guide, ClippedAdam({\"lr\":1e-2}), loss=Trace_ELBO())\n",
    "\n",
    "for step in range(500):\n",
    "    loss = svi.step(data[\"lsoa_idx\"], data[\"X\"], data[\"y\"])\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step:4d}  ELBO loss = {loss:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a57fb",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d00ae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training columns: ['imd_rank', 'Income Rank (where 1 is most deprived)', 'Employment Rank (where 1 is most deprived)', 'Education, Skills and Training Rank (where 1 is most deprived)', 'Health Deprivation and Disability Rank (where 1 is most deprived)', 'Crime Rank (where 1 is most deprived)', 'Barriers to Housing and Services Rank (where 1 is most deprived)', 'Living Environment Rank (where 1 is most deprived)', 'Education locations', 'Emergency locations', 'Entertainment locations', 'Food locations', 'Leisure locations', 'Parking locations', 'Shopping locations', 'Public transport locations', 'Dwelling type|Flat, maisonette or apartment (%)', 'Ethnic Group|Asian/Asian British (%)', 'Ethnic Group|BAME (%)', 'Ethnic Group|Black/African/Caribbean/Black British (%)', 'Ethnic Group|Mixed/multiple ethnic groups (%)', 'Ethnic Group|Other ethnic group (%)', 'Ethnic Group|White (%)', 'Household Composition|% Couple household with dependent children', 'Household Composition|% Couple household without dependent children', 'Household Composition|% Lone parent household', 'Household Composition|% One person household', 'Household Composition|% Other multi person household', 'Households|All households', 'Mid-year Population Estimates|Aged 0-15', 'Mid-year Population Estimates|Aged 16-29', 'Mid-year Population Estimates|Aged 30-44', 'Mid-year Population Estimates|Aged 45-64', 'Mid-year Population Estimates|Aged 65+', 'Mid-year Population Estimates|All Ages', 'Mid-year Population Estimates|Working-age', 'Tenure|Owned outright (%)', 'Tenure|Owned with a mortgage or loan (%)', 'Tenure|Private rented (%)', 'Tenure|Social rented (%)', 'Car or van availability|1 car or van in household (%)', 'Car or van availability|2 cars or vans in household (%)', 'Car or van availability|3 cars or vans in household (%)', 'Car or van availability|4 or more cars or vans in household (%)', 'Car or van availability|Cars per household', 'Car or van availability|No cars or vans in household (%)', 'Public Transport Accessibility Levels|% 0-1 (poor access)|Level3_65', 'Public Transport Accessibility Levels|% 2-3 (average access)|Level3_66', 'Public Transport Accessibility Levels|% 4-6 (good access)|Level3_67', 'Public Transport Accessibility Levels|Average Score|Level3_64', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|0', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|1a', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|1b', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|2', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|3', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|4', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|5', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|6a', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|6b', 'time_s', 'month_sin', 'month_cos', 'prev_month', 'prev_year']\n",
      "Prediction columns: ['imd_rank', 'Income Rank (where 1 is most deprived)', 'Employment Rank (where 1 is most deprived)', 'Education, Skills and Training Rank (where 1 is most deprived)', 'Health Deprivation and Disability Rank (where 1 is most deprived)', 'Crime Rank (where 1 is most deprived)', 'Barriers to Housing and Services Rank (where 1 is most deprived)', 'Living Environment Rank (where 1 is most deprived)', 'Education locations', 'Emergency locations', 'Entertainment locations', 'Food locations', 'Leisure locations', 'Parking locations', 'Shopping locations', 'Public transport locations', 'Dwelling type|Flat, maisonette or apartment (%)', 'Ethnic Group|Asian/Asian British (%)', 'Ethnic Group|BAME (%)', 'Ethnic Group|Black/African/Caribbean/Black British (%)', 'Ethnic Group|Mixed/multiple ethnic groups (%)', 'Ethnic Group|Other ethnic group (%)', 'Ethnic Group|White (%)', 'Household Composition|% Couple household with dependent children', 'Household Composition|% Couple household without dependent children', 'Household Composition|% Lone parent household', 'Household Composition|% One person household', 'Household Composition|% Other multi person household', 'Households|All households', 'Mid-year Population Estimates|Aged 0-15', 'Mid-year Population Estimates|Aged 16-29', 'Mid-year Population Estimates|Aged 30-44', 'Mid-year Population Estimates|Aged 45-64', 'Mid-year Population Estimates|Aged 65+', 'Mid-year Population Estimates|All Ages', 'Mid-year Population Estimates|Working-age', 'Tenure|Owned outright (%)', 'Tenure|Owned with a mortgage or loan (%)', 'Tenure|Private rented (%)', 'Tenure|Social rented (%)', 'Car or van availability|1 car or van in household (%)', 'Car or van availability|2 cars or vans in household (%)', 'Car or van availability|3 cars or vans in household (%)', 'Car or van availability|4 or more cars or vans in household (%)', 'Car or van availability|Cars per household', 'Car or van availability|No cars or vans in household (%)', 'Public Transport Accessibility Levels|% 0-1 (poor access)|Level3_65', 'Public Transport Accessibility Levels|% 2-3 (average access)|Level3_66', 'Public Transport Accessibility Levels|% 4-6 (good access)|Level3_67', 'Public Transport Accessibility Levels|Average Score|Level3_64', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|0', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|1a', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|1b', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|2', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|3', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|4', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|5', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|6a', 'Public Transport Accessibility Levels|Number of people in each PTAL level:|6b', 'time_s', 'month_sin', 'month_cos', 'prev_month', 'prev_year', 'pred_mean', 'pred_5pct', 'pred_95pct', 'predicted']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (68) must match the size of tensor b (64) at non-singleton dimension 1\nTrace Shapes:          \n Param Sites:          \nSample Sites:          \n    mu_a dist      |   \n        value      |   \n sigma_a dist      |   \n        value      |   \n      ls dist      |   \n        value 4994 |   \n       a dist 4994 |   \n        value 4994 |   \n  b_attr dist      | 64\n        value      | 64\n    data dist      |   \n        value 4994 |   ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\pyro\\poutine\\trace_messenger.py:191\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\pyro\\poutine\\messenger.py:32\u001b[0m, in \u001b[0;36m_context_wrap\u001b[1;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(lidx, x)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Predictive\n\u001b[0;32m      7\u001b[0m predictive \u001b[38;5;241m=\u001b[39m Predictive(\n\u001b[1;32m----> 8\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m lidx, x: \u001b[43mburglary_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_subsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[0;32m      9\u001b[0m     guide\u001b[38;5;241m=\u001b[39mguide,\n\u001b[0;32m     10\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     11\u001b[0m     return_sites\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# This returns a dict of tensors; obs has shape [1000, num_locations]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 75\u001b[0m, in \u001b[0;36mburglary_model\u001b[1;34m(lsoa_idx, X, y, use_subsample)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyro\u001b[38;5;241m.\u001b[39mplate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, N):\n\u001b[1;32m---> 75\u001b[0m     eta \u001b[38;5;241m=\u001b[39m a[lsoa_idx] \u001b[38;5;241m+\u001b[39m (\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb_attr\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m     mu  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(eta\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\torch\\utils\\_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (68) must match the size of tensor b (64) at non-singleton dimension 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 15\u001b[0m\n\u001b[0;32m      7\u001b[0m predictive \u001b[38;5;241m=\u001b[39m Predictive(\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m lidx, x: burglary_model(lidx, x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_subsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m      9\u001b[0m     guide\u001b[38;5;241m=\u001b[39mguide,\n\u001b[0;32m     10\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     11\u001b[0m     return_sites\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# This returns a dict of tensors; obs has shape [1000, num_locations]\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m pred_samples \u001b[38;5;241m=\u001b[39m \u001b[43mpredictive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlsoa_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\pyro\\infer\\predictive.py:277\u001b[0m, in \u001b[0;36mPredictive.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m     return_sites \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_sites \u001b[38;5;28;01melse\u001b[39;00m return_sites\n\u001b[0;32m    268\u001b[0m     posterior_samples \u001b[38;5;241m=\u001b[39m _predictive(\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguide,\n\u001b[0;32m    270\u001b[0m         posterior_samples,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m    276\u001b[0m     )\u001b[38;5;241m.\u001b[39msamples\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_predictive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposterior_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_sites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_sites\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\pyro\\infer\\predictive.py:89\u001b[0m, in \u001b[0;36m_predictive\u001b[1;34m(model, posterior_samples, num_samples, return_sites, parallel, model_args, model_kwargs, mask)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_predictive\u001b[39m(\n\u001b[0;32m     79\u001b[0m     model,\n\u001b[0;32m     80\u001b[0m     posterior_samples,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m     mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     87\u001b[0m ):\n\u001b[0;32m     88\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad()(poutine\u001b[38;5;241m.\u001b[39mmask(model, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;28;01melse\u001b[39;00m model)\n\u001b[1;32m---> 89\u001b[0m     max_plate_nesting \u001b[38;5;241m=\u001b[39m \u001b[43m_guess_max_plate_nesting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     vectorize \u001b[38;5;241m=\u001b[39m pyro\u001b[38;5;241m.\u001b[39mplate(\n\u001b[0;32m     91\u001b[0m         _predictive_vectorize_plate_name, num_samples, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mmax_plate_nesting \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     92\u001b[0m     )\n\u001b[0;32m     93\u001b[0m     model_trace \u001b[38;5;241m=\u001b[39m prune_subsample_sites(\n\u001b[0;32m     94\u001b[0m         poutine\u001b[38;5;241m.\u001b[39mtrace(model)\u001b[38;5;241m.\u001b[39mget_trace(\u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m     95\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\pyro\\infer\\predictive.py:26\u001b[0m, in \u001b[0;36m_guess_max_plate_nesting\u001b[1;34m(model, args, kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03mGuesses max_plate_nesting by running the model once\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03mwithout enumeration. This optimistically assumes static model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03mstructure.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mblock():\n\u001b[1;32m---> 26\u001b[0m     model_trace \u001b[38;5;241m=\u001b[39m poutine\u001b[38;5;241m.\u001b[39mtrace(model)\u001b[38;5;241m.\u001b[39mget_trace(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     27\u001b[0m sites \u001b[38;5;241m=\u001b[39m [site \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m model_trace\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     29\u001b[0m dims \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     30\u001b[0m     frame\u001b[38;5;241m.\u001b[39mdim\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m sites\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m site[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcond_indep_stack\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mvectorized\n\u001b[0;32m     34\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\pyro\\poutine\\trace_messenger.py:216\u001b[0m, in \u001b[0;36mTraceHandler.get_trace\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Trace:\n\u001b[0;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m    :returns: data structure\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    :rtype: pyro.poutine.Trace\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Calls this poutine and returns its trace instead of the function's return value.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mget_trace()\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\pyro\\poutine\\trace_messenger.py:198\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m         exc \u001b[38;5;241m=\u001b[39m exc_type(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(exc_value, shapes))\n\u001b[0;32m    197\u001b[0m         exc \u001b[38;5;241m=\u001b[39m exc\u001b[38;5;241m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m--> 198\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_RETURN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mret\n\u001b[0;32m    201\u001b[0m     )\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\pyro\\poutine\\trace_messenger.py:191\u001b[0m, in \u001b[0;36mTraceHandler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsngr\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39madd_node(\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_INPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    193\u001b[0m     exc_type, exc_value, traceback \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\pyro\\poutine\\messenger.py:32\u001b[0m, in \u001b[0;36m_context_wrap\u001b[1;34m(context, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_context_wrap\u001b[39m(\n\u001b[0;32m     26\u001b[0m     context: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessenger\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     fn: Callable,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;241m*\u001b[39margs: Any,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     30\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[1;32m---> 32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(lidx, x)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, next_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeat_cols\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Predictive\n\u001b[0;32m      7\u001b[0m predictive \u001b[38;5;241m=\u001b[39m Predictive(\n\u001b[1;32m----> 8\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m lidx, x: \u001b[43mburglary_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_subsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[0;32m      9\u001b[0m     guide\u001b[38;5;241m=\u001b[39mguide,\n\u001b[0;32m     10\u001b[0m     num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[0;32m     11\u001b[0m     return_sites\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# This returns a dict of tensors; obs has shape [1000, num_locations]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m pred_samples \u001b[38;5;241m=\u001b[39m predictive(next_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlsoa_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m], next_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[32], line 75\u001b[0m, in \u001b[0;36mburglary_model\u001b[1;34m(lsoa_idx, X, y, use_subsample)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m pyro\u001b[38;5;241m.\u001b[39mplate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, N):\n\u001b[1;32m---> 75\u001b[0m         eta \u001b[38;5;241m=\u001b[39m a[lsoa_idx] \u001b[38;5;241m+\u001b[39m (\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb_attr\u001b[49m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m         mu  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(eta\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     77\u001b[0m         pyro\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, dist\u001b[38;5;241m.\u001b[39mPoisson(mu), obs\u001b[38;5;241m=\u001b[39my \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rayan\\anaconda3\\envs\\dcpyro\\lib\\site-packages\\torch\\utils\\_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (68) must match the size of tensor b (64) at non-singleton dimension 1\nTrace Shapes:          \n Param Sites:          \nSample Sites:          \n    mu_a dist      |   \n        value      |   \n sigma_a dist      |   \n        value      |   \n      ls dist      |   \n        value 4994 |   \n       a dist 4994 |   \n        value 4994 |   \n  b_attr dist      | 64\n        value      | 64\n    data dist      |   \n        value 4994 |   "
     ]
    }
   ],
   "source": [
    "next_data = load_and_prepare(next_month)\n",
    "print(\"Training columns:\", data[\"feat_cols\"])\n",
    "print(\"Prediction columns:\", next_data[\"feat_cols\"])\n",
    "\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "predictive = Predictive(\n",
    "    model=lambda lidx, x: burglary_model(lidx, x, y=None, use_subsample=False),\n",
    "    guide=guide,\n",
    "    num_samples=1000,\n",
    "    return_sites=[\"obs\"]\n",
    ")\n",
    "\n",
    "# This returns a dict of tensors; obs has shape [1000, num_locations]\n",
    "pred_samples = predictive(next_data[\"lsoa_idx\"], next_data[\"X\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11df7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean = pred_samples[\"obs\"].float().mean(dim=0)\n",
    "pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12949d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = pred_samples[\"obs\"].kthvalue(int(0.05 * 1000), dim=0).values\n",
    "upper = pred_samples[\"obs\"].kthvalue(int(0.95 * 1000), dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bde65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_month[\"pred_mean\"]    = pred_mean.cpu().numpy()\n",
    "next_month[\"pred_5pct\"]    = lower.cpu().numpy()\n",
    "next_month[\"pred_95pct\"]   = upper.cpu().numpy()\n",
    "next_month[\"predicted\"] = (\n",
    "    (next_month[\"burglaries\"] > next_month[\"pred_5pct\"])\n",
    "    & \n",
    "    (next_month[\"burglaries\"] < next_month[\"pred_95pct\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_month[[\"lsoa_idx\", \"burglaries\", \"pred_mean\", \"pred_5pct\", \"pred_95pct\", \"predicted\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcc7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Faster optimizer\n",
    "svi = SVI(burglary_model, guide, ClippedAdam({\"lr\": 5e-2}), loss=Trace_ELBO())\n",
    "\n",
    "# 2. Profile to measure gains\n",
    "%timeit svi.step(data[\"lsoa_idx\"], data[\"X\"], data[\"y\"])\n",
    "\n",
    "\n",
    "# 3. Check for batching possibility (if dataset is large)\n",
    "with pyro.plate(\"data\", len(y), subsample_size=512) as i:\n",
    "    pyro.sample(\"obs\", dist.Poisson(mu[i]), obs=y[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcpyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
